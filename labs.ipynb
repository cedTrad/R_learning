{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.action_space.sample()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for e in range(1, 200):\n",
    "    a = env.action_space.sample()\n",
    "    state, reward, done, info, _ = env.step(a)\n",
    "    \n",
    "    print(f\"step ={e}  state={state}  action={a} done={done}  reward={reward}\")\n",
    "    if done and (e + 1) < 200:\n",
    "        print(\"**Failed**\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, w):\n",
    "    s = np.dot(state, w)\n",
    "    if s < 0:\n",
    "        a = 0\n",
    "    else:\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "def run_episode(env, w):\n",
    "    state = env.reset()[0]\n",
    "    treward = 0\n",
    "    for i in range(200):\n",
    "        a = policy(state, w)\n",
    "        state, reward, done, info, _ = env.step(a)\n",
    "        treward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return treward\n",
    "\n",
    "w = np.random.random(4) * 2 - 1\n",
    "run_episode(env, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = 100\n",
    "besttreward = 0\n",
    "\n",
    "for e in range(1, num_eps+1):\n",
    "    \n",
    "    w = np.random.rand(4) * 2 - 1\n",
    "    treward = run_episode(env, w)\n",
    "    \n",
    "    if treward > besttreward:\n",
    "        besttreward = treward\n",
    "        bestweights = w\n",
    "        \n",
    "        if treward == 200:\n",
    "            print(f\"SUCCESS   episode={e}\")\n",
    "            break\n",
    "        print(f\"UPDATE   episode={e}\")\n",
    "    \n",
    "print(f\"besttreward : {besttreward}   bestweights : {bestweights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "w = bestweights\n",
    "for _ in range(200):\n",
    "    treward = run_episode(env, w)\n",
    "    res.append(treward)\n",
    "\n",
    "np.mean(res)\n",
    "np.std(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    num_step = 1000\n",
    "    w = np.random.rand(4) * 2 - 1\n",
    "    \n",
    "    for i in range(num_step):\n",
    "        action = policy(state, w)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        episode.append(\n",
    "            (state, action, reward)\n",
    "        )\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "episode = generate_episode(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = zip(*episode)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "total_return = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    episode = generate_episode(policy)\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    \n",
    "    for t, state in enumerate(states):\n",
    "        R = (sum(rewards[t:]))\n",
    "        total_return[state] = total_return[state] + R\n",
    "        N[state] = N[state] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "ENV_NAME = \"FrozenLake8x8-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "#n=\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(\n",
    "            collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "        \n",
    "    \n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "            \n",
    "    \n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "    \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_return\n",
    "    \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action) \n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, r, done, _, prob = env.step(0)\n",
    "\n",
    "(s, 0, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import collections\n",
    "\n",
    "rewards = collections.defaultdict(float)\n",
    "transits = collections.defaultdict(\n",
    "            collections.Counter)\n",
    "values = collections.defaultdict(float)\n",
    "\n",
    "\n",
    "\n",
    "ENV_NAME = \"FrozenLake8x8-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "state = env.reset()[0]\n",
    "for i in range(iterations):\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, is_done, info, prob = env.step(action)\n",
    "    \n",
    "    rewards[(state, action, new_state)] = reward\n",
    "    transits[(state, action)][new_state] += 1\n",
    "    state = new_state\n",
    "    \n",
    "    if is_done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(transits[(0, 3)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = env.P\n",
    "\n",
    "st[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackJack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return 0 if state[0] > 15 else 1\n",
    "\n",
    "\n",
    "def generate_episode(policy):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    num_timestep = 100\n",
    "    \n",
    "    for i in range(num_timestep):        \n",
    "        action = policy(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        episode.append(\n",
    "            (state, action, reward)\n",
    "        )\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "episode = generate_episode(policy)\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    episode = generate_episode(policy)\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    \n",
    "    for t, state in enumerate(states):\n",
    "        R = (sum(rewards[t:]))\n",
    "        total_return[state] = total_return[state] + R\n",
    "        N[state] = N[state] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = pd.DataFrame(total_return.items(),columns=['state', 'total_return'])\n",
    "total_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM\n",
    "\n",
    "\n",
    "env  = gym.make(\"CartPole-v1\")\n",
    "#env  = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "#n=\n",
    "\n",
    "class NNAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max = 0\n",
    "        self.scores = list()\n",
    "        self.memory = list()\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(24, input_dim = 4, activation = 'relu')\n",
    "        )\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = RMSprop(lr = 0.001)\n",
    "                      )\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= 0.5:\n",
    "            return env.action_space.sample()\n",
    "        action= np.where(self.model.predict(state, batch_size = None)[0, 0] > 0.5, 1, 0)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def train_model(self, state, action):\n",
    "        self.model.fit(state, np.array([action,]),\n",
    "                       epochs = 1, verbose = False)\n",
    "        \n",
    "    \n",
    "    def learn(self, episode):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            for i in range(201):\n",
    "                state = np.reshape(state, [1, 4])\n",
    "                action= self.act(state)\n",
    "                next_state, reward, done, info, _ = env.step(action)\n",
    "                if done:\n",
    "                    score = i + 1\n",
    "                    self.scores.append(score)\n",
    "                    self.max = max(score, self.max)\n",
    "                    print(\"episode : {i}  score:{self.scores}  max:{self.max}\")\n",
    "                    break\n",
    "            self.memory.append(\n",
    "                (state, action)\n",
    "            )\n",
    "            self.train_model(state, action)\n",
    "            state = next_state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NNAgent()\n",
    "episodes = 10\n",
    "\n",
    "agent.learn(episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "class DQLAgent:\n",
    "    \n",
    "    def __init__(self, gamma = 0.95, hu = 24, opt = Adam, lr = 0.001,\n",
    "                 finish = False):\n",
    "        self.finish = finish\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 32\n",
    "        self.max_treward = 0\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.osn = env.observation_space.shape[0]\n",
    "        self.model = self.build_model(hu, opt, lr)\n",
    "        \n",
    "        \n",
    "    def build_model(self, hu, opt, lr):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(hu, input_dim = self.osn, activation = 'relu')\n",
    "        )\n",
    "        model.add(\n",
    "            Dense(hu, activation = 'relu')\n",
    "        )\n",
    "        model.add(\n",
    "            Dense(env.action_space.n, activation = 'linear')\n",
    "        )\n",
    "        model.compile(loss = 'mse', optimizer = opt(lr = lr))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state)[0]\n",
    "        return np.argmax(action)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0]\n",
    "                )\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs = 1, verbose = False)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    \n",
    "            \n",
    "    def learn(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            state = np.reshape(state, [1, self.osn])\n",
    "            \n",
    "            for i in range(500):\n",
    "                action= self.act(state)\n",
    "                next_state, reward, done, info, _ = env.step(action)\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, self.osn])\n",
    "                self.memory.append([state, action, reward, next_state, done])\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    treward = i + 1\n",
    "                    trewards.append(treward)\n",
    "                    \n",
    "                    av = sum(trewards[-25:]) / 25\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    \n",
    "                    templ = \"episode : {}/{}  -  treward:{}\"\n",
    "                    templ += \"av : {}  -  max : {}\"\n",
    "                    print(templ.format(e, episodes, treward, av, self.max_treward), \n",
    "                          end = '\\r')\n",
    "                    break\n",
    "            \n",
    "            if av > 195 and self.finish:\n",
    "                break\n",
    "            \n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            for i in range(200):\n",
    "                state = np.reshape(state, [1, self.osn])\n",
    "                action = np.argmax(self.model.predict(state)[0])\n",
    "                next_state, reward, done, info, _ = env.step(action)\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    treward = i + 1\n",
    "                    trewards.append(treward)\n",
    "                    print(\"episode : {}/{}  -  treward : {}\".format(e, episodes, treward), end = '\\r')\n",
    "                    break\n",
    "        return trewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30\n",
    "\n",
    "agent = DQLAgent(finish=True)\n",
    "agent.learn(episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "x = range(len(agent.averages))\n",
    "y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(len(agent.averages))\n",
    "y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(agent.averages,label = \"moving average\")\n",
    "plt.plot(x, y, 'r--', label ='trend')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('total reward')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treward = agent.test(episodes)\n",
    "treward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(treward) / len(treward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
