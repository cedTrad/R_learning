{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gym\n",
    "import multiprocessing\n",
    "import threading\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the mountain car environment\n",
    "\n",
    "Let's create a mountain car environment using the gym. Note that our mountain car\n",
    "environment is a continuous environment meaning that our action space is continuous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = env.observation_space.shape[0]\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_shape = env.action_space.shape[0]\n",
    "action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.], dtype=float32), array([1.], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "action_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the variables\n",
    "\n",
    "\n",
    "Now, let's define some of the important variables.\n",
    "\n",
    "Define the number of workers as the number of CPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = multiprocessing.cpu_count() \n",
    "num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000 \n",
    "num_timesteps = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global network ( global agent) scopte\n",
    "global_net_scope = 'Global_Net'\n",
    "\n",
    "# The time step at which we want to update the global network\n",
    "update_global = 10\n",
    "\n",
    "# discount factr \n",
    "gamma = 0.9\n",
    "\n",
    "# beta\n",
    "beta = 0.01\n",
    "\n",
    "# The directory where we want to store the logs\n",
    "log_dir = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the actor critic class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(scope):\n",
    "    return 10, 5, 4, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = \"o\"\n",
    "global_net_scope = \"o\"\n",
    "\n",
    "actor_optimizer = tf.train.RMSPropOptimizer(0.0001, name=\"RMSPropA\")\n",
    "critic_optimizer = tf.train.RMSPropOptimizer(0.0001, name=\"RMSPropC\")\n",
    "\n",
    "\n",
    "if scope == global_net_scope:\n",
    "    with tf.variable_scope(scope):\n",
    "        state = tf.placeholder(tf.float32, [None, state_shape])\n",
    "        actor_params, critic_params = build_network(scope)[-2:]\n",
    "        \n",
    "else:\n",
    "    with tf.variable_scope(scope):\n",
    "        state = tf.placeholder(tf.float32, [None, state_shape], \"state\")\n",
    "        action_dist = tf.placeholder(tf.float32, [None, action_shape], \"action\")\n",
    "        target_value = tf.placeholder(tf.float32, [None, 1], \"Vtarget\")\n",
    "        \n",
    "        mean, variance, value, actor_params, critic_params = build_network(scope)\n",
    "        \n",
    "        td_error = tf.subtract(target_value, value, name=\"TD_error\")\n",
    "        \n",
    "        with tf.name_scope(\"critic_loss\"):\n",
    "            critic_loss = tf.reduce_mean(tf.square(td_error))\n",
    "        \n",
    "        with tf.name_scope(\"wrap_action\"):\n",
    "            mean, variance = mean * action_bound[1], variance + 1e-4\n",
    "        \n",
    "        normal_dist = tf.distributions.Normal(mean, variance)\n",
    "        \n",
    "        with tf.name_scope(\"actor_loss\"):\n",
    "            log_prob = normal_dist.log_prob(action_dist)\n",
    "            entropy_pi = normal_dist.entropy()\n",
    "            \n",
    "            loss = log_prob * td_error + (beta * entropy_pi)\n",
    "            action_loss = tf.reduce_mean(-loss)\n",
    "        \n",
    "        with tf.name_scope(\"select_action\"):\n",
    "            action = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0),\n",
    "                                      action_bound[0], action_bound[1])\n",
    "        \n",
    "        with tf.name_scope(\"local_grad\"):\n",
    "            actor_grads = tf.gradients(actor_loss, actor_params)\n",
    "            critic_grads = tf.gradients(critic_loss, critic_params)\n",
    "            \n",
    "    with tf.name_scope(\"sync\"):\n",
    "        \n",
    "        with tf.name_scope(\"push\"):\n",
    "            update_actor_params = actor_optimizer.apply_gradients(zip(actor_grads,\n",
    "                                                                      globalAC.actor_params))\n",
    "            update_critic_params = critic_optimizer.apply_gradients(zip(actor_grads,\n",
    "                                                                        globalAC.critic_params))\n",
    "            \n",
    "        with tf.name_scope(\"pull\"):\n",
    "            pull_actor_params = [l_p.assign(g_p) for l_p, g_p in zip(actor_params,\n",
    "                                                                     globalAC.actor_params)]\n",
    "            pull_critic_params = [l_p.assign(g_p) for l_p, g_p in zip(critic_params,\n",
    "                                                                      globalAC.critic_params)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `fetch` = <built-in function sum> has invalid type \"builtin_function_or_method\" must be a string or Tensor. (Can not convert a builtin_function_or_method into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:304\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39;49mget_default_graph()\u001b[39m.\u001b[39;49mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_operation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3998\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3997\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3998\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4086\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   4084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# We give up!\u001b[39;00m\n\u001b[1;32m-> 4086\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not convert a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m into a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   4087\u001b[0m                   (\u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, types_str))\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a builtin_function_or_method into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cc\\Desktop\\CedAlgo\\R_Learning\\AC2_AC3.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cc/Desktop/CedAlgo/R_Learning/AC2_AC3.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m sess:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cc/Desktop/CedAlgo/R_Learning/AC2_AC3.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mFileWriter(\u001b[39m\"\u001b[39m\u001b[39m./graphs\u001b[39m\u001b[39m\"\u001b[39m, sess\u001b[39m.\u001b[39mgraph)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cc/Desktop/CedAlgo/R_Learning/AC2_AC3.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(sess\u001b[39m.\u001b[39;49mrun(\u001b[39msum\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1176\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       feed_map[compat\u001b[39m.\u001b[39mas_bytes(subfeed_t\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[0;32m   1175\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[1;32m-> 1176\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[0;32m   1177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39;49mfeed_handles)\n\u001b[0;32m   1179\u001b[0m \u001b[39m# Run request and get response.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m \u001b[39m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[39m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[39m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:485\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \n\u001b[0;32m    475\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m    direct feeds.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 485\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_mapper \u001b[39m=\u001b[39m _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetches)\n\u001b[0;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39m=\u001b[39m []\n\u001b[0;32m    487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_targets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:276\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, tensor_type):\n\u001b[0;32m    275\u001b[0m       fetches, contraction_fn \u001b[39m=\u001b[39m fetch_fn(fetch)\n\u001b[1;32m--> 276\u001b[0m       \u001b[39mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[0;32m    277\u001b[0m \u001b[39m# Did not find anything.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    279\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:307\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_operation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 307\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    308\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m must be a string or Tensor. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    309\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m cannot be interpreted as \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    312\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma Tensor. (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument `fetch` = <built-in function sum> has invalid type \"builtin_function_or_method\" must be a string or Tensor. (Can not convert a builtin_function_or_method into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph)\n",
    "    print(sess.run(sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
