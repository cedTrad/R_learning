{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Frozen Lake Problem with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "    \n",
    "    num_iterations = 1000\n",
    "    threshold = 1e-20\n",
    "    gamma = 1.0\n",
    "    \n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        for s in range(env.observation_space.n):\n",
    "            Q_values = [\n",
    "                sum([prob * (r + gamma*updated_value_table[s_]) for prob, s_, r, _ in env.P[s][a]])\n",
    "                for a in range(env.action_space.n)\n",
    "                ]\n",
    "            \n",
    "            value_table[s] = max(Q_values)\n",
    "            \n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 2, 2, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum([a, b]) for a, b in zip(range(2), range(2)) for _ in range(3) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting optimal policy from the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    \n",
    "    gamma = 1.0\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        Q_values = [sum(\n",
    "            [prob * (r + gamma*value_table[s_])\n",
    "             for prob, s_, r, _ in env.P[s][a]])\n",
    "                    for a in range(env.action_space.n)\n",
    "                    ]\n",
    "        policy[s] = np.argmax(np.array(Q_values))\n",
    "    \n",
    "    return policy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82352941, 0.82352941, 0.82352941, 0.82352941, 0.82352941,\n",
       "       0.        , 0.52941176, 0.        , 0.82352941, 0.82352941,\n",
       "       0.76470588, 0.        , 0.        , 0.88235294, 0.94117647,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_value_function = value_iteration(env)\n",
    "optimal_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing value function using policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(policy):\n",
    "    num_iterations = 1000\n",
    "    threshold = 1e-20\n",
    "    gamma = 1.0\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        for s in range(env.observation_space.n):\n",
    "            a = policy[s]\n",
    "            value_table[s] = sum([prob * (r + gamma * updated_value_table[s_])\n",
    "                                  for prob, s_, r, _ in env.P[s][a]\n",
    "                                  ])\n",
    "        \n",
    "        if (np.sum((np.fabs(updated_value_table - value_table))) < threshold):\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "    \n",
    "    num_iterations = 1000\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        value_function = compute_value_function(policy)\n",
    "        new_policy = extract_policy(value_function)\n",
    "        \n",
    "        if (np.all(policy == new_policy)):\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = policy_iteration(env)\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return 0 if state[0] > 15 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = state[0][0]\n",
    "dealer = state[0][1]\n",
    "usable = state[0][2]\n",
    "\n",
    "print(f\"player : {player}   dealer : {dealer}   usable : {usable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    num_timestep = 100\n",
    "    \n",
    "    for i in range(num_timestep):        \n",
    "        action = policy(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        episode.append(\n",
    "            (state, action, reward)\n",
    "        )\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "episode = generate_episode(policy)\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = zip(*episode)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    episode = generate_episode(policy)\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    \n",
    "    for t, state in enumerate(states):\n",
    "        R = (sum(rewards[t:]))\n",
    "        total_return[state] = total_return[state] + R\n",
    "        N[state] = N[state] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = pd.DataFrame(total_return.items(),columns=['state', 'total_return'])\n",
    "total_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pd.DataFrame(N.items(),columns=['state', 'N'])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(total_return, N, on=\"state\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'] = df['total_return']/df['N']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "num_iterations = 10000\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    episode = generate_episode(policy)\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    \n",
    "    for t, state in enumerate(states):\n",
    "        \n",
    "        if state not in states[0:t]:    \n",
    "            R = (sum(rewards[t:]))\n",
    "            total_return[state] = total_return[state] + R\n",
    "            \n",
    "            N[state] = N[state] + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = pd.DataFrame(total_return.items(),columns=['state', 'total_return'])\n",
    "N = pd.DataFrame(N.items(),columns=['state', 'N'])\n",
    "\n",
    "df = pd.merge(total_return, N, on=\"state\")\n",
    "df['value'] = df['total_return']/df['N']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing On-policy MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)\n",
    "total_return = defaultdict(float)\n",
    "N = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q):\n",
    "    \n",
    "    epsilon = 0.5\n",
    "    \n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(\n",
    "            list(range(env.action_space.n)), key = lambda x : Q[(state, x)]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100\n",
    "\n",
    "def generate_episode(Q):\n",
    "    \n",
    "    episode = env.reset()\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        action = epsilon_greedy_policy(state, Q)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append(\n",
    "            (state, action, reward)\n",
    "        )\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cc\\miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozenlake Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(\n",
    "            collections.Counter\n",
    "        )\n",
    "        self.values = collections.defaultdict(float)\n",
    "    \n",
    "    \n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action, new_state)] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "\n",
    "agent.play_n_random_steps(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_env.action_space.sample()\n",
    "test_env.step(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = (88, 80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_rewards = np.zeros_like(episode_rewards)\n",
    "    reward_to_go = 0.0\n",
    "    \n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        reward_to_go = reward_to_go * gamma + episode_rewards[i]\n",
    "        discounted_rewards[i] = reward_to_go\n",
    "        \n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= np.std(discounted_rewards)\n",
    "    \n",
    "    return discounted_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ph = tf.placeholder(tf.float32, [None, state_shape], name=\"state_ph\")\n",
    "\n",
    "action_ph = tf.placeholder(tf.int32, [None, num_actions],  name=\"action_ph\")\n",
    "\n",
    "discounted_rewards_ph = tf.placeholder(tf.float32, [None,], name=\"discounted_rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.layers.dense(state_ph, units = 32, activation=tf.nn.relu)\n",
    "\n",
    "layer2 = tf.layers.dense(layer1, units=num_actions)\n",
    "\n",
    "prob_dist = tf.nn.softmax(layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer2, labels=action_ph)\n",
    "\n",
    "loss = tf.reduce_mean(neg_log_policy * discounted_rewards_ph)\n",
    "\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Return : 10.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `fetch` = 0.2018943727016449 has invalid type \"float32\" must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:304\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39;49mget_default_graph()\u001b[39m.\u001b[39;49mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_operation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3998\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3997\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3998\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4086\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   4084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# We give up!\u001b[39;00m\n\u001b[1;32m-> 4086\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not convert a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m into a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   4087\u001b[0m                   (\u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, types_str))\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a float32 into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     57\u001b[0m feed_dict \u001b[39m=\u001b[39m {state_ph : np\u001b[39m.\u001b[39mvstack(np\u001b[39m.\u001b[39marray(episode_states)),\n\u001b[0;32m     58\u001b[0m              action_ph : np\u001b[39m.\u001b[39mvstack(np\u001b[39m.\u001b[39marray(episode_actions)),\n\u001b[0;32m     59\u001b[0m              discounted_rewards_ph : discounted_rewards\n\u001b[0;32m     60\u001b[0m              }\n\u001b[0;32m     62\u001b[0m \u001b[39m# train the network\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m loss, _ \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun([loss, train], feed_dict \u001b[39m=\u001b[39;49m feed_dict)\n\u001b[0;32m     65\u001b[0m \u001b[39m# print the return for every 10 iteration\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1176\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       feed_map[compat\u001b[39m.\u001b[39mas_bytes(subfeed_t\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[0;32m   1175\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[1;32m-> 1176\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[0;32m   1177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39;49mfeed_handles)\n\u001b[0;32m   1179\u001b[0m \u001b[39m# Run request and get response.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m \u001b[39m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[39m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[39m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:485\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \n\u001b[0;32m    475\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m    direct feeds.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 485\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_mapper \u001b[39m=\u001b[39m _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetches)\n\u001b[0;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39m=\u001b[39m []\n\u001b[0;32m    487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_targets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:266\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    263\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Cannot be None\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    265\u001b[0m   \u001b[39m# NOTE(touts): This is also the code path for namedtuples.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m   \u001b[39mreturn\u001b[39;00m _ListFetchMapper(fetch)\n\u001b[0;32m    267\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, collections_abc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    268\u001b[0m   \u001b[39mreturn\u001b[39;00m _DictFetchMapper(fetch)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:378\u001b[0m, in \u001b[0;36m_ListFetchMapper.__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(fetches)\n\u001b[1;32m--> 378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers \u001b[39m=\u001b[39m [_FetchMapper\u001b[39m.\u001b[39mfor_fetch(fetch) \u001b[39mfor\u001b[39;00m fetch \u001b[39min\u001b[39;00m fetches]\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_indices \u001b[39m=\u001b[39m _uniquify_fetches(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(fetches)\n\u001b[1;32m--> 378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers \u001b[39m=\u001b[39m [_FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetch) \u001b[39mfor\u001b[39;00m fetch \u001b[39min\u001b[39;00m fetches]\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_indices \u001b[39m=\u001b[39m _uniquify_fetches(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:276\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, tensor_type):\n\u001b[0;32m    275\u001b[0m       fetches, contraction_fn \u001b[39m=\u001b[39m fetch_fn(fetch)\n\u001b[1;32m--> 276\u001b[0m       \u001b[39mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[0;32m    277\u001b[0m \u001b[39m# Did not find anything.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    279\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cc\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:307\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_operation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 307\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    308\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m must be a string or Tensor. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    309\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m cannot be interpreted as \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    312\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma Tensor. (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument `fetch` = 0.2018943727016449 has invalid type \"float32\" must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "\n",
    "# start the TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initialize all the TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        # initialize an empty list for storing the states, actions, and rewards obtained in the episode\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        \n",
    "        # set the done to False\n",
    "        done = False\n",
    "        \n",
    "        # initialize the state by resetting the environment\n",
    "        state = env.reset()[0]\n",
    "        \n",
    "        # initialize the return\n",
    "        Return=0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # reshape the state\n",
    "            state = state.reshape([1, 4])\n",
    "            \n",
    "            pi = sess.run(prob_dist, feed_dict = {state_ph : state})\n",
    "            \n",
    "            # select an action using this stochastic policy\n",
    "            a = np.random.choice(range(pi.shape[1]), p=pi.ravel())\n",
    "            \n",
    "            # perform the selected action\n",
    "            next_state, reward, done, info, _ = env.step(a)\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            # update the return\n",
    "            Return += reward\n",
    "            \n",
    "            # one-hot encode the action\n",
    "            action = np.zeros(num_actions)\n",
    "            action[a] = 1\n",
    "            \n",
    "            # store the state, action, and reward into their respective list\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            # update the state to the next state\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute the discounted and normalized reward\n",
    "        discounted_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "        \n",
    "        # define the feed dictionary\n",
    "        feed_dict = {state_ph : np.vstack(np.array(episode_states)),\n",
    "                     action_ph : np.vstack(np.array(episode_actions)),\n",
    "                     discounted_rewards_ph : discounted_rewards\n",
    "                     }\n",
    "        \n",
    "        # train the network\n",
    "        loss, _ = sess.run([loss, train], feed_dict = feed_dict)\n",
    "        \n",
    "        # print the return for every 10 iteration\n",
    "        if i%10 == 0:\n",
    "            print(\"Iteration : {}, Return : {}\".format(i, Return))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
